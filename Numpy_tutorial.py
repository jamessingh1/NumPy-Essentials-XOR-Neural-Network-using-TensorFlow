# -*- coding: utf-8 -*-
"""Numpy_Tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Va1QBs7H-o9EclL75YjlGIZYoPlTJHAS

What is NumPy?

NumPy (Numerical Python) is a powerful library used for:

Fast numerical computations

Handling large multi-dimensional arrays

Useful in AI/ML, data analysis, and scientific computing
"""

import numpy as np

# Create a 1D array
a = np.array([1, 2, 3])
print("1D Array:", a)

# Create a 2D array (matrix)
b = np.array([[1, 2], [3, 4],[10,12]])
print("2D Array:\n", b)

print("Shape:", b.shape)
print("Size:", b.size)
print("Datatype:", b.dtype)

zeros = np.zeros((2, 3))      # 2x3 array of zeros
zeros

ones = np.ones((2, 3))
ones

randoms = np.random.rand(2, 2)
randoms

range_array = np.arange(0, 10, 2)
range_array

x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

print("Addition:", x + y)
print("Multiplication:", x * y)
print("Dot Product:", np.dot(x, y))

arr = np.array([10, 20, 30, 40, 50])

print(arr[0])      # First element
print(arr[1:4])    # Slice from index 1 to 3

# 1D, 2D, 3D arrays
a = np.array([1, 2, 3])
b = np.array([[1, 2], [3, 4]])
c = np.array([[[1], [2]], [[3], [4]]])

print("1D:", a)
print("2D:\n", b)
print("3D:\n", c)

np.random.randint(1, 10, (2, 3))

"""Reshape & Flatten"""

arr = np.array([[1, 2, 3], [4, 5, 6]])
print("Reshape to 3x2:\n", arr.reshape(3, 2))
print("Flatten:", arr.flatten())

a = np.array([1, 2])
b = np.array([3, 4])

print("Vertical Stack:\n", np.vstack((a, b)))
print("Horizontal Stack:\n", np.hstack((a, b)))

arr = np.array([10, 20, 30, 40, 50])
print("Greater than 30:", arr[arr > 30])

##Useful NumPy Functions

arr = np.array([[1, 2, 3], [4, 5, 6]])

print("Sum:", np.sum(arr))
print("Max:", np.max(arr))
print("Mean:", np.mean(arr))
print("Std Dev:", np.std(arr))
print("Argmax:", np.argmax(arr))

a = np.array([[1], [2], [3]])
b = np.array([10, 20, 30])

# Automatically expands shapes to match
print("Broadcasted Addition:\n", a + b)

# Q: Create a 3x3 matrix with values from 1 to 9 and find the sum of its diagonal
mat = np.arange(1, 10).reshape(3, 3)
print("Matrix:\n", mat)
print("Diagonal Sum:", np.trace(mat))

"""Goal of the Code

We're training an Artificial Neural Network (ANN) to learn the XOR logic gate, which cannot be solved by a single-layer perceptron (because it's not linearly separable). We’ll use Keras + TensorFlow for that.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

"""tensorflow: The deep learning library used to build and train neural networks.

Sequential: A linear stack of layers (input → hidden → output).

Dense: Fully connected layer.
"""

import numpy as np

X = np.array([[0,0], [0,1], [1,0], [1,1]])  # Input data
y = np.array([0, 1, 1, 0])                  # Output (XOR of inputs)

"""XOR Truth Table (Explained for Slide)
XOR stands for "Exclusive OR".

It returns 1 if exactly one of the inputs is 1.

Returns 0 if both inputs are the same (either 0 and 0 or 1 and 1).

You can explain like this with bullet points:

✅ Input: 0, 0 → Output: 0
✅ Input: 0, 1 → Output: 1
✅ Input: 1, 0 → Output: 1
✅ Input: 1, 1 → Output: 0
"""

##Build the ANN Model
model = Sequential() #You're using a Sequential model, meaning layers will be added one after the other.

model.add(Dense(4, input_dim=2, activation='relu'))  # Hidden Layer

"""Dense(4): Adds 4 neurons in the hidden layer.

input_dim=2: Input has 2 features (since each XOR input is a pair).

activation='relu': ReLU introduces non-linearity.

Why 4 neurons? Just a small hidden layer to learn patterns. Enough for XOR, which needs at least one hidden layer.
"""

model.add(Dense(1, activation='sigmoid'))

"""Dense(1): Output layer has 1 neuron (since XOR gives 0 or 1).

activation='sigmoid': Good for binary classification (outputs between 0 and 1).
"""

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, epochs=500, verbose=0)

"""compile() → tells the model how to learn

fit() → trains the model using input and output data

loss → measures error

optimizer → adjusts weights

metrics → tracks performance

epochs → how many times to train

verbose → shows training output

"""

